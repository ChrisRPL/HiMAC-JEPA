{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models Training\n",
    "\n",
    "This notebook provides an interactive interface for training all baseline models.\n",
    "\n",
    "**Baselines:**\n",
    "1. Camera-Only (ResNet18 + LSTM)\n",
    "2. LiDAR-Only (PointNet++)\n",
    "3. Radar-Only (3D CNN)\n",
    "4. I-JEPA (ViT + JEPA, camera-only)\n",
    "5. V-JEPA (Multi-modal JEPA)\n",
    "\n",
    "**Usage:**\n",
    "- Run cells sequentially\n",
    "- Modify hyperparameters as needed\n",
    "- Monitor training progress with progress bars\n",
    "- Results saved to `checkpoints/baselines/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import baseline models\n",
    "from src.models.baselines import (\n",
    "    CameraOnlyBaseline,\n",
    "    LiDAROnlyBaseline,\n",
    "    RadarOnlyBaseline,\n",
    "    IJEPABaseline,\n",
    "    VJEPABaseline\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline models imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which baselines to train\n",
    "BASELINES_TO_TRAIN = [\n",
    "    'camera_only',\n",
    "    'lidar_only',\n",
    "    'radar_only',\n",
    "    'ijepa',\n",
    "    'vjepa'\n",
    "]\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS = 50  # Set lower for notebook (use 100 for full training)\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "USE_WANDB = False  # Set to True to enable W&B logging\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Baselines: {BASELINES_TO_TRAIN}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(model_name: str) -> dict:\n",
    "    \"\"\"Load configuration for a baseline model.\"\"\"\n",
    "    config_path = project_root / f\"configs/baseline/{model_name}.yaml\"\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Override with notebook settings\n",
    "    config['training']['num_epochs'] = NUM_EPOCHS\n",
    "    config['training']['batch_size'] = BATCH_SIZE\n",
    "    config['training']['learning_rate'] = LEARNING_RATE\n",
    "    config['logging']['wandb']['enabled'] = USE_WANDB\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def create_model(model_name: str, config: dict):\n",
    "    \"\"\"Create baseline model from config.\"\"\"\n",
    "    if model_name == 'camera_only':\n",
    "        model = CameraOnlyBaseline(config['model'])\n",
    "    elif model_name == 'lidar_only':\n",
    "        model = LiDAROnlyBaseline(config['model'])\n",
    "    elif model_name == 'radar_only':\n",
    "        model = RadarOnlyBaseline(config['model'])\n",
    "    elif model_name == 'ijepa':\n",
    "        model = IJEPABaseline(config['model'])\n",
    "    elif model_name == 'vjepa':\n",
    "        model = VJEPABaseline(config['model'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dummy_dataloader(model_name: str, batch_size: int, num_batches: int = 100):\n",
    "    \"\"\"Create dummy dataloader for testing (replace with actual dataset).\"\"\"\n",
    "    \n",
    "    class DummyDataset:\n",
    "        def __init__(self, model_name, num_samples):\n",
    "            self.model_name = model_name\n",
    "            self.num_samples = num_samples\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            batch = {}\n",
    "            \n",
    "            if self.model_name in ['camera_only', 'ijepa']:\n",
    "                batch['camera'] = torch.randn(3, 224, 224)\n",
    "            elif self.model_name == 'lidar_only':\n",
    "                batch['lidar'] = torch.randn(2048, 3)\n",
    "            elif self.model_name == 'radar_only':\n",
    "                batch['radar'] = torch.randn(1, 128, 128)\n",
    "            elif self.model_name == 'vjepa':\n",
    "                # Temporal data for V-JEPA\n",
    "                batch['camera'] = torch.randn(5, 3, 224, 224)  # (T, C, H, W)\n",
    "                batch['lidar'] = torch.randn(5, 2048, 3)\n",
    "                batch['radar'] = torch.randn(5, 1, 128, 128)\n",
    "            \n",
    "            return batch\n",
    "    \n",
    "    dataset = DummyDataset(model_name, num_batches * batch_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model_name: str, config: dict, device: str):\n",
    "    \"\"\"Train a single baseline model.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(model_name, config)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {model.get_num_parameters():,}\")\n",
    "    print(f\"Model size: {model.get_model_size_mb():.2f} MB\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['training']['learning_rate'],\n",
    "        weight_decay=config['training']['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['training']['num_epochs'] - config['training']['warmup_epochs'],\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders (using dummy data - replace with actual dataset)\n",
    "    train_loader = create_dummy_dataloader(model_name, config['training']['batch_size'], num_batches=50)\n",
    "    val_loader = create_dummy_dataloader(model_name, config['training']['batch_size'], num_batches=10)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'lr': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, config['training']['num_epochs'] + 1):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{config['training']['num_epochs']}\")\n",
    "        for batch in pbar:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            # Training step\n",
    "            metrics = model.train_step(batch, optimizer)\n",
    "            train_losses.append(metrics['loss'])\n",
    "            \n",
    "            pbar.set_postfix(loss=metrics['loss'])\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "                metrics = model.val_step(batch)\n",
    "                val_losses.append(metrics['loss'])\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Update scheduler\n",
    "        if epoch > config['training']['warmup_epochs']:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}, lr={optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            save_dir = project_root / config['checkpoint']['save_dir']\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            model.save_checkpoint(\n",
    "                str(save_dir / 'best_model.pth'),\n",
    "                epoch=epoch,\n",
    "                optimizer=optimizer\n",
    "            )\n",
    "            print(f\"  ✓ Saved best model (val_loss={best_val_loss:.4f})\")\n",
    "    \n",
    "    print(f\"\\n✓ Training complete for {model_name}\")\n",
    "    print(f\"  Best val loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store training histories\n",
    "all_histories = {}\n",
    "\n",
    "for model_name in BASELINES_TO_TRAIN:\n",
    "    try:\n",
    "        # Load config\n",
    "        config = load_config(model_name)\n",
    "        \n",
    "        # Train\n",
    "        history = train_baseline(model_name, config, DEVICE)\n",
    "        all_histories[model_name] = history\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error training {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All baselines training complete!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Training loss\n",
    "for model_name, history in all_histories.items():\n",
    "    axes[0].plot(history['train_loss'], label=model_name, linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "for model_name, history in all_histories.items():\n",
    "    axes[1].plot(history['val_loss'], label=model_name, linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Validation Loss Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "for model_name, history in all_histories.items():\n",
    "    axes[2].plot(history['lr'], label=model_name, linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_title('Learning Rate Schedule')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(project_root / 'results/baseline_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training curves saved to results/baseline_training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for model_name, history in all_histories.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name,\n",
    "        'Final Train Loss': history['train_loss'][-1],\n",
    "        'Final Val Loss': history['val_loss'][-1],\n",
    "        'Best Val Loss': min(history['val_loss']),\n",
    "        'Best Epoch': np.argmin(history['val_loss']) + 1\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Best Val Loss')\n",
    "\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_df.to_csv(project_root / 'results/baseline_training_summary.csv', index=False)\n",
    "print(\"\\n✓ Summary saved to results/baseline_training_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that baselines are trained:\n",
    "1. Run `02_evaluate_baselines.ipynb` to evaluate models on test set\n",
    "2. Run `03_results_analysis.ipynb` to analyze and compare results\n",
    "3. Run `04_visualize_predictions.ipynb` for qualitative visualization\n",
    "\n",
    "**Checkpoints saved to:**\n",
    "- `checkpoints/baselines/camera_only/best_model.pth`\n",
    "- `checkpoints/baselines/lidar_only/best_model.pth`\n",
    "- `checkpoints/baselines/radar_only/best_model.pth`\n",
    "- `checkpoints/baselines/ijepa/best_model.pth`\n",
    "- `checkpoints/baselines/vjepa/best_model.pth`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
