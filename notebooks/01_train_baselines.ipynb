{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models + HiMAC-JEPA Training\n",
    "\n",
    "This notebook provides an interactive interface for training all baseline models AND HiMAC-JEPA.\n",
    "\n",
    "**Models:**\n",
    "1. Camera-Only (ResNet18 + LSTM)\n",
    "2. LiDAR-Only (PointNet++)\n",
    "3. Radar-Only (3D CNN)\n",
    "4. I-JEPA (ViT + JEPA, camera-only)\n",
    "5. V-JEPA (Multi-modal JEPA)\n",
    "6. **HiMAC-JEPA** (Multi-modal + Hierarchical Actions + JEPA)\n",
    "\n",
    "**Usage:**\n",
    "- Run cells sequentially\n",
    "- Modify hyperparameters as needed\n",
    "- Monitor training progress with progress bars\n",
    "- Results saved to checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import baseline models\n",
    "from src.models.baselines import (\n",
    "    CameraOnlyBaseline,\n",
    "    LiDAROnlyBaseline,\n",
    "    RadarOnlyBaseline,\n",
    "    IJEPABaseline,\n",
    "    VJEPABaseline\n",
    ")\n",
    "\n",
    "# Import HiMAC-JEPA\n",
    "from src.models.himac_jepa import HiMACJEPA\n",
    "\n",
    "print(\"✓ All models imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which models to train\n",
    "MODELS_TO_TRAIN = [\n",
    "    'camera_only',\n",
    "    'lidar_only',\n",
    "    'radar_only',\n",
    "    'ijepa',\n",
    "    'vjepa',\n",
    "    'himac_jepa'  # Add HiMAC-JEPA\n",
    "]\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS = 50  # Set lower for notebook (use 100 for full training)\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "USE_WANDB = False  # Set to True to enable W&B logging\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Models: {MODELS_TO_TRAIN}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def load_config(model_name: str) -> dict:\n    \"\"\"Load configuration for a baseline model or HiMAC-JEPA.\"\"\"\n    if model_name == 'himac_jepa':\n        config_path = project_root / \"configs/config.yaml\"\n    else:\n        config_path = project_root / f\"configs/baseline/{model_name}.yaml\"\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    # Override with notebook settings\n    config['training']['num_epochs'] = NUM_EPOCHS\n    config['training']['batch_size'] = BATCH_SIZE\n    config['training']['learning_rate'] = LEARNING_RATE\n    config['logging']['wandb']['enabled'] = USE_WANDB\n\n    return config\n\n\ndef create_model(model_name: str, config: dict):\n    \"\"\"Create baseline model from config.\"\"\"\n    if model_name == 'camera_only':\n        model = CameraOnlyBaseline(config['model'])\n    elif model_name == 'lidar_only':\n        model = LiDAROnlyBaseline(config['model'])\n    elif model_name == 'radar_only':\n        model = RadarOnlyBaseline(config['model'])\n    elif model_name == 'ijepa':\n        model = IJEPABaseline(config['model'])\n    elif model_name == 'vjepa':\n        model = VJEPABaseline(config['model'])\n    elif model_name == 'himac_jepa':\n        model = HiMACJEPA(config['model'])\n    else:\n        raise ValueError(f\"Unknown model: {model_name}\")\n\n    return model\n\n\ndef create_dummy_dataloader(model_name: str, batch_size: int, num_batches: int = 100):\n    \"\"\"Create dummy dataloader for testing (replace with actual dataset).\"\"\"\n\n    class DummyDataset:\n        def __init__(self, model_name, num_samples):\n            self.model_name = model_name\n            self.num_samples = num_samples\n\n        def __len__(self):\n            return self.num_samples\n\n        def __getitem__(self, idx):\n            batch = {}\n\n            if self.model_name in ['camera_only', 'ijepa']:\n                batch['camera'] = torch.randn(3, 224, 224)\n            elif self.model_name == 'lidar_only':\n                batch['lidar'] = torch.randn(2048, 3)\n            elif self.model_name == 'radar_only':\n                batch['radar'] = torch.randn(1, 128, 128)\n            elif self.model_name == 'vjepa':\n                # Temporal data for V-JEPA\n                batch['camera'] = torch.randn(5, 3, 224, 224)  # (T, C, H, W)\n                batch['lidar'] = torch.randn(5, 2048, 3)\n                batch['radar'] = torch.randn(5, 1, 128, 128)\n            elif self.model_name == 'himac_jepa':\n                # Multi-modal + hierarchical actions for HiMAC-JEPA\n                batch['camera'] = torch.randn(5, 3, 224, 224)  # (T, C, H, W)\n                batch['lidar'] = torch.randn(5, 2048, 3)\n                batch['radar'] = torch.randn(5, 1, 128, 128)\n                batch['strategic_actions'] = torch.randn(5, 4)  # (T, strategic_dim)\n                batch['tactical_actions'] = torch.randn(5, 8)   # (T, tactical_dim)\n\n            return batch\n\n    dataset = DummyDataset(model_name, num_batches * batch_size)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n\n    return dataloader\n\n\nprint(\"✓ Helper functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_baseline(model_name: str, config: dict, device: str):\n    \"\"\"Train a single baseline model.\"\"\"\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Training: {model_name.upper()}\")\n    print(f\"{'='*60}\")\n\n    # Create model\n    model = create_model(model_name, config)\n    model = model.to(device)\n\n    print(f\"Model parameters: {model.get_num_parameters():,}\")\n    print(f\"Model size: {model.get_model_size_mb():.2f} MB\")\n\n    # Create optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=config['training']['learning_rate'],\n        weight_decay=config['training']['weight_decay']\n    )\n\n    # Create scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=config['training']['num_epochs'] - config['training']['warmup_epochs'],\n        eta_min=1e-6\n    )\n\n    # Create dataloaders (using dummy data - replace with actual dataset)\n    train_loader = create_dummy_dataloader(model_name, config['training']['batch_size'], num_batches=50)\n    val_loader = create_dummy_dataloader(model_name, config['training']['batch_size'], num_batches=10)\n\n    # Training history\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'lr': []\n    }\n\n    # Training loop\n    best_val_loss = float('inf')\n\n    for epoch in range(1, config['training']['num_epochs'] + 1):\n        # Train\n        model.train()\n        train_losses = []\n\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{config['training']['num_epochs']}\")\n        for batch in pbar:\n            # Move to device\n            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n\n            # Training step\n            metrics = model.train_step(batch, optimizer)\n            train_losses.append(metrics['loss'])\n\n            pbar.set_postfix(loss=metrics['loss'])\n\n        avg_train_loss = np.mean(train_losses)\n\n        # Validate\n        model.eval()\n        val_losses = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                metrics = model.val_step(batch)\n                val_losses.append(metrics['loss'])\n\n        avg_val_loss = np.mean(val_losses)\n\n        # Update scheduler\n        if epoch > config['training']['warmup_epochs']:\n            scheduler.step()\n\n        # Save history\n        history['train_loss'].append(avg_train_loss)\n        history['val_loss'].append(avg_val_loss)\n        history['lr'].append(optimizer.param_groups[0]['lr'])\n\n        # Print progress\n        print(f\"Epoch {epoch}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}, lr={optimizer.param_groups[0]['lr']:.6f}\")\n\n        # Save best model\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            # HiMAC-JEPA uses different checkpoint directory\n            if model_name == 'himac_jepa':\n                save_dir = project_root / 'checkpoints/himac_jepa'\n            else:\n                save_dir = project_root / config['checkpoint']['save_dir']\n            save_dir.mkdir(parents=True, exist_ok=True)\n            model.save_checkpoint(\n                str(save_dir / 'best_model.pth'),\n                epoch=epoch,\n                optimizer=optimizer\n            )\n            print(f\"  ✓ Saved best model (val_loss={best_val_loss:.4f})\")\n\n    print(f\"\\n✓ Training complete for {model_name}\")\n    print(f\"  Best val loss: {best_val_loss:.4f}\")\n\n    return history\n\n\nprint(\"✓ Training function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train All Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Store training histories\nall_histories = {}\n\nfor model_name in MODELS_TO_TRAIN:\n    try:\n        # Load config\n        config = load_config(model_name)\n\n        # Train\n        history = train_baseline(model_name, config, DEVICE)\n        all_histories[model_name] = history\n\n    except Exception as e:\n        print(f\"\\n❌ Error training {model_name}: {e}\")\n        import traceback\n        traceback.print_exc()\n        continue\n\nprint(f\"\\n{'='*60}\")\nprint(\"All model training complete!\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Training loss\nfor model_name, history in all_histories.items():\n    axes[0].plot(history['train_loss'], label=model_name, linewidth=2)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Training Loss')\naxes[0].set_title('Training Loss Curves')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Validation loss\nfor model_name, history in all_histories.items():\n    axes[1].plot(history['val_loss'], label=model_name, linewidth=2)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Validation Loss')\naxes[1].set_title('Validation Loss Curves')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# Learning rate\nfor model_name, history in all_histories.items():\n    axes[2].plot(history['lr'], label=model_name, linewidth=2)\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Learning Rate')\naxes[2].set_title('Learning Rate Schedule')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\naxes[2].set_yscale('log')\n\nplt.tight_layout()\nplt.savefig(project_root / 'results/training_curves.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"✓ Training curves saved to results/training_curves.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Create summary table\nsummary_data = []\nfor model_name, history in all_histories.items():\n    summary_data.append({\n        'Model': model_name,\n        'Final Train Loss': history['train_loss'][-1],\n        'Final Val Loss': history['val_loss'][-1],\n        'Best Val Loss': min(history['val_loss']),\n        'Best Epoch': np.argmin(history['val_loss']) + 1\n    })\n\nsummary_df = pd.DataFrame(summary_data)\nsummary_df = summary_df.sort_values('Best Val Loss')\n\nprint(\"\\nTraining Summary:\")\nprint(summary_df.to_string(index=False))\n\n# Save summary\nsummary_df.to_csv(project_root / 'results/training_summary.csv', index=False)\nprint(\"\\n✓ Summary saved to results/training_summary.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nNow that all models are trained:\n1. Run `02_evaluate_baselines.ipynb` to evaluate models on test set\n2. Run `03_results_analysis.ipynb` to analyze and compare results\n3. Run `04_visualize_predictions.ipynb` for qualitative visualization\n\n**Checkpoints saved to:**\n- `checkpoints/baselines/camera_only/best_model.pth`\n- `checkpoints/baselines/lidar_only/best_model.pth`\n- `checkpoints/baselines/radar_only/best_model.pth`\n- `checkpoints/baselines/ijepa/best_model.pth`\n- `checkpoints/baselines/vjepa/best_model.pth`\n- `checkpoints/himac_jepa/best_model.pth`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}