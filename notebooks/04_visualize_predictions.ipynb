{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Prediction Visualization\n",
    "\n",
    "This notebook provides qualitative visualization of baseline model predictions.\n",
    "\n",
    "**Visualizations:**\n",
    "1. Trajectory predictions (predicted vs ground truth)\n",
    "2. BEV segmentation masks\n",
    "3. Motion predictions for surrounding agents\n",
    "4. Multi-modal latent representations (t-SNE/UMAP)\n",
    "5. Attention maps (for V-JEPA/I-JEPA)\n",
    "\n",
    "**Usage:**\n",
    "- Run cells sequentially\n",
    "- Visualizations saved to `results/baselines/visualizations/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Circle, Rectangle, FancyArrow\n",
    "from matplotlib.collections import PatchCollection\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style(\"white\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import baseline models\nfrom src.models.baselines import (\n    CameraOnlyBaseline,\n    LiDAROnlyBaseline,\n    RadarOnlyBaseline,\n    IJEPABaseline,\n    VJEPABaseline\n)\n\n# Import HiMAC-JEPA\nfrom src.models.himac_jepa import HiMACJEPA\n\nprint(\"✓ All models imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Output directory\nvis_dir = project_root / 'results/baselines/visualizations'\nvis_dir.mkdir(parents=True, exist_ok=True)\n\n# Device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Models to visualize\nMODELS = ['camera_only', 'lidar_only', 'ijepa', 'vjepa', 'himac_jepa']\n\nprint(f\"Visualization directory: {vis_dir}\")\nprint(f\"Device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trajectory Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_dummy_trajectories(num_models=5, horizon_steps=30):\n    \"\"\"Generate dummy trajectory predictions for visualization.\"\"\"\n\n    # Ground truth trajectory (smooth curve)\n    t = np.linspace(0, 3, horizon_steps)\n    gt_x = t * 10 + np.sin(t * 2) * 2\n    gt_y = t * 5 + np.cos(t * 3) * 1.5\n\n    # Predicted trajectories (with varying accuracy)\n    predictions = {}\n    noise_levels = [0.15, 0.3, 0.5, 0.8, 1.2]  # Increasing noise for different models\n\n    model_names = ['himac_jepa', 'vjepa', 'ijepa', 'camera_only', 'lidar_only']\n\n    for i, (model_name, noise) in enumerate(zip(model_names[:num_models], noise_levels)):\n        pred_x = gt_x + np.random.normal(0, noise, horizon_steps).cumsum() * 0.1\n        pred_y = gt_y + np.random.normal(0, noise, horizon_steps).cumsum() * 0.1\n        predictions[model_name] = (pred_x, pred_y)\n\n    return (gt_x, gt_y), predictions\n\n\n# Generate trajectories\ngt_traj, pred_trajs = generate_dummy_trajectories()\ngt_x, gt_y = gt_traj\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 10))\n\n# Plot ground truth\nax.plot(gt_x, gt_y, 'k-', linewidth=4, label='Ground Truth', zorder=10)\nax.plot(gt_x[0], gt_y[0], 'go', markersize=15, label='Start', zorder=11)\nax.plot(gt_x[-1], gt_y[-1], 'r*', markersize=20, label='Goal', zorder=11)\n\n# Plot predictions\ncolors = ['#8c564b', '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\nlinestyles = ['-', '-', '--', '-.', ':']\n\nfor i, (model_name, (pred_x, pred_y)) in enumerate(pred_trajs.items()):\n    ax.plot(pred_x, pred_y, linestyle=linestyles[i], color=colors[i], \n           linewidth=2.5, label=f'{model_name} prediction', alpha=0.8)\n\n    # Show final position error\n    final_error = np.sqrt((pred_x[-1] - gt_x[-1])**2 + (pred_y[-1] - gt_y[-1])**2)\n    ax.plot(pred_x[-1], pred_y[-1], 'o', color=colors[i], markersize=10)\n\n# Styling\nax.set_xlabel('X Position (m)', fontsize=14, fontweight='bold')\nax.set_ylabel('Y Position (m)', fontsize=14, fontweight='bold')\nax.set_title('Trajectory Predictions Comparison (3s Horizon)', fontsize=16, fontweight='bold')\nax.legend(fontsize=12, loc='upper left', frameon=True, shadow=True)\nax.grid(True, alpha=0.3, linestyle='--')\nax.axis('equal')\n\nplt.tight_layout()\nplt.savefig(vis_dir / 'trajectory_predictions.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"✓ Saved to {vis_dir / 'trajectory_predictions.png'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BEV Segmentation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_dummy_bev(size=(200, 200)):\n    \"\"\"Generate dummy BEV segmentation masks.\"\"\"\n\n    # Classes: 0=background, 1=road, 2=lane, 3=vehicle, 4=pedestrian\n    H, W = size\n\n    # Ground truth\n    gt = np.zeros((H, W), dtype=np.uint8)\n    gt[50:150, 80:120] = 1  # Road\n    gt[70:130, 98:102] = 2  # Lane marking\n    gt[60:80, 85:95] = 3    # Vehicle\n    gt[120:140, 105:115] = 4 # Pedestrian\n\n    # Predictions (with varying quality)\n    predictions = {}\n\n    # HiMAC-JEPA (best)\n    pred = gt.copy()\n    noise_mask = np.random.rand(H, W) > 0.97\n    pred[noise_mask] = np.random.randint(0, 5, noise_mask.sum())\n    predictions['himac_jepa'] = pred\n\n    # V-JEPA (very good)\n    pred = gt.copy()\n    noise_mask = np.random.rand(H, W) > 0.95\n    pred[noise_mask] = np.random.randint(0, 5, noise_mask.sum())\n    predictions['vjepa'] = pred\n\n    # I-JEPA (good)\n    pred = gt.copy()\n    noise_mask = np.random.rand(H, W) > 0.90\n    pred[noise_mask] = np.random.randint(0, 5, noise_mask.sum())\n    predictions['ijepa'] = pred\n\n    # Camera-only (medium)\n    pred = gt.copy()\n    noise_mask = np.random.rand(H, W) > 0.80\n    pred[noise_mask] = np.random.randint(0, 5, noise_mask.sum())\n    predictions['camera_only'] = pred\n\n    # LiDAR-only (poor semantics)\n    pred = gt.copy()\n    noise_mask = np.random.rand(H, W) > 0.60\n    pred[noise_mask] = np.random.randint(0, 5, noise_mask.sum())\n    predictions['lidar_only'] = pred\n\n    return gt, predictions\n\n\n# Generate BEV masks\ngt_bev, pred_bevs = generate_dummy_bev()\n\n# Color map for classes\ncmap_colors = [\n    [0.2, 0.2, 0.2],    # Background - dark gray\n    [0.5, 0.5, 0.5],    # Road - gray\n    [1.0, 1.0, 0.0],    # Lane - yellow\n    [0.0, 0.0, 1.0],    # Vehicle - blue\n    [1.0, 0.0, 0.0],    # Pedestrian - red\n]\nfrom matplotlib.colors import ListedColormap\ncmap = ListedColormap(cmap_colors)\n\n# Plot\nfig, axes = plt.subplots(1, 6, figsize=(24, 4))\n\n# Ground truth\naxes[0].imshow(gt_bev, cmap=cmap, vmin=0, vmax=4)\naxes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\naxes[0].axis('off')\n\n# Predictions\nfor idx, (model_name, pred) in enumerate(pred_bevs.items()):\n    axes[idx+1].imshow(pred, cmap=cmap, vmin=0, vmax=4)\n\n    # Compute IoU (simplified)\n    iou = (pred == gt_bev).mean()\n\n    axes[idx+1].set_title(f'{model_name}\\nIoU: {iou:.3f}', fontsize=14, fontweight='bold')\n    axes[idx+1].axis('off')\n\n# Add legend\nfrom matplotlib.patches import Patch\nlegend_elements = [\n    Patch(facecolor=cmap_colors[0], label='Background'),\n    Patch(facecolor=cmap_colors[1], label='Road'),\n    Patch(facecolor=cmap_colors[2], label='Lane'),\n    Patch(facecolor=cmap_colors[3], label='Vehicle'),\n    Patch(facecolor=cmap_colors[4], label='Pedestrian'),\n]\nfig.legend(handles=legend_elements, loc='lower center', ncol=5, fontsize=12, frameon=True)\n\nplt.suptitle('BEV Segmentation Predictions', fontsize=16, fontweight='bold', y=0.98)\nplt.tight_layout(rect=[0, 0.05, 1, 0.95])\nplt.savefig(vis_dir / 'bev_segmentation.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(f\"✓ Saved to {vis_dir / 'bev_segmentation.png'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Agent Motion Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_multi_agent_scene(num_agents=5):\n",
    "    \"\"\"Generate dummy multi-agent motion prediction scene.\"\"\"\n",
    "    \n",
    "    # Ego vehicle\n",
    "    ego_pos = np.array([0, 0])\n",
    "    ego_heading = 0  # radians\n",
    "    \n",
    "    # Surrounding agents (position, heading, velocity)\n",
    "    agents = []\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for i in range(num_agents):\n",
    "        pos = np.random.uniform(-20, 20, 2)\n",
    "        heading = np.random.uniform(0, 2*np.pi)\n",
    "        velocity = np.random.uniform(2, 10)\n",
    "        agents.append({'pos': pos, 'heading': heading, 'velocity': velocity})\n",
    "    \n",
    "    # Generate future trajectories (3s, 0.5s steps = 6 steps)\n",
    "    dt = 0.5\n",
    "    steps = 6\n",
    "    \n",
    "    trajectories = {}\n",
    "    for i, agent in enumerate(agents):\n",
    "        traj = [agent['pos']]\n",
    "        pos = agent['pos'].copy()\n",
    "        heading = agent['heading']\n",
    "        velocity = agent['velocity']\n",
    "        \n",
    "        for step in range(steps):\n",
    "            # Simple constant velocity model + slight curve\n",
    "            heading += np.random.normal(0, 0.05)\n",
    "            pos += np.array([np.cos(heading), np.sin(heading)]) * velocity * dt\n",
    "            traj.append(pos.copy())\n",
    "        \n",
    "        trajectories[i] = np.array(traj)\n",
    "    \n",
    "    return ego_pos, agents, trajectories\n",
    "\n",
    "\n",
    "# Generate scene\n",
    "ego_pos, agents, trajectories = generate_dummy_multi_agent_scene()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# Plot ego vehicle\n",
    "ego_rect = Rectangle(ego_pos - [2, 1], 4, 2, angle=0, \n",
    "                     facecolor='green', edgecolor='black', linewidth=2, label='Ego Vehicle')\n",
    "ax.add_patch(ego_rect)\n",
    "ax.plot(ego_pos[0], ego_pos[1], 'g*', markersize=20, zorder=10)\n",
    "\n",
    "# Plot agents and trajectories\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(agents)))\n",
    "\n",
    "for i, (agent, traj) in enumerate(zip(agents, trajectories.values())):\n",
    "    pos = agent['pos']\n",
    "    heading = agent['heading']\n",
    "    \n",
    "    # Agent bounding box\n",
    "    rect = Rectangle(pos - [1.5, 0.75], 3, 1.5, \n",
    "                     angle=np.degrees(heading),\n",
    "                     facecolor=colors[i], edgecolor='black', \n",
    "                     linewidth=1.5, alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Current position\n",
    "    ax.plot(pos[0], pos[1], 'o', color=colors[i], markersize=10, zorder=5)\n",
    "    \n",
    "    # Predicted trajectory\n",
    "    ax.plot(traj[:, 0], traj[:, 1], '--', color=colors[i], \n",
    "           linewidth=2, alpha=0.8, label=f'Agent {i+1}')\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'x', color=colors[i], \n",
    "           markersize=12, markeredgewidth=3, zorder=5)\n",
    "\n",
    "# Styling\n",
    "ax.set_xlabel('X Position (m)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Y Position (m)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Multi-Agent Motion Prediction (3s Horizon)', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper right', frameon=True, shadow=True)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.axis('equal')\n",
    "ax.set_xlim([-30, 30])\n",
    "ax.set_ylim([-30, 30])\n",
    "\n",
    "# Add reference circle\n",
    "circle = Circle((0, 0), 20, fill=False, edgecolor='gray', \n",
    "               linestyle=':', linewidth=2, alpha=0.5, label='20m range')\n",
    "ax.add_patch(circle)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(vis_dir / 'multi_agent_motion.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Saved to {vis_dir / 'multi_agent_motion.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Space Visualization (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "try:\n    from sklearn.manifold import TSNE\n    tsne_available = True\nexcept ImportError:\n    print(\"⚠️  scikit-learn not available, skipping t-SNE visualization\")\n    tsne_available = False\n\nif tsne_available:\n    # Generate dummy latent representations\n    np.random.seed(42)\n\n    num_samples = 200\n    latent_dim = 256\n\n    # Simulate latent representations from different models\n    model_latents = {}\n\n    for i, model_name in enumerate(['himac_jepa', 'vjepa', 'ijepa', 'camera_only', 'lidar_only']):\n        # Each model has slightly different distribution\n        center = np.random.randn(latent_dim) * 0.5\n        latents = np.random.randn(num_samples, latent_dim) * 0.3 + center\n        model_latents[model_name] = latents\n\n    # Combine all latents\n    all_latents = np.vstack(list(model_latents.values()))\n    all_labels = np.concatenate([np.full(num_samples, i) \n                                 for i in range(len(model_latents))])\n\n    # Apply t-SNE\n    print(\"Running t-SNE (this may take a moment)...\")\n    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n    latents_2d = tsne.fit_transform(all_latents)\n\n    # Plot\n    fig, ax = plt.subplots(figsize=(12, 10))\n\n    colors = ['#8c564b', '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n    model_names = list(model_latents.keys())\n\n    for i, (model_name, color) in enumerate(zip(model_names, colors)):\n        mask = all_labels == i\n        ax.scatter(latents_2d[mask, 0], latents_2d[mask, 1], \n                  c=color, label=model_name, s=50, alpha=0.6, edgecolors='black', linewidth=0.5)\n\n    ax.set_xlabel('t-SNE Dimension 1', fontsize=14, fontweight='bold')\n    ax.set_ylabel('t-SNE Dimension 2', fontsize=14, fontweight='bold')\n    ax.set_title('Latent Space Visualization (t-SNE)', fontsize=16, fontweight='bold')\n    ax.legend(fontsize=12, frameon=True, shadow=True, loc='best')\n    ax.grid(True, alpha=0.3, linestyle='--')\n\n    plt.tight_layout()\n    plt.savefig(vis_dir / 'latent_space_tsne.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(f\"✓ Saved to {vis_dir / 'latent_space_tsne.png'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*20 + \"VISUALIZATION SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"Generated visualizations:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"  1. Trajectory predictions     → {vis_dir / 'trajectory_predictions.png'}\")\n",
    "print(f\"  2. BEV segmentation          → {vis_dir / 'bev_segmentation.png'}\")\n",
    "print(f\"  3. Multi-agent motion        → {vis_dir / 'multi_agent_motion.png'}\")\n",
    "if tsne_available:\n",
    "    print(f\"  4. Latent space (t-SNE)      → {vis_dir / 'latent_space_tsne.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nAll visualizations saved to:\", vis_dir)\n",
    "print(\"\\nThese are dummy visualizations for demonstration.\")\n",
    "print(\"Replace with actual model predictions for real results.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Visualization complete!\n",
    "\n",
    "**To use with real data:**\n",
    "1. Replace dummy data generation with actual dataset samples\n",
    "2. Load trained models and extract predictions\n",
    "3. Visualize real predictions vs ground truth\n",
    "\n",
    "**All notebook suite complete:**\n",
    "- ✅ `01_train_baselines.ipynb` - Training\n",
    "- ✅ `02_evaluate_baselines.ipynb` - Evaluation\n",
    "- ✅ `03_results_analysis.ipynb` - Analysis & plots\n",
    "- ✅ `04_visualize_predictions.ipynb` - Qualitative visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}