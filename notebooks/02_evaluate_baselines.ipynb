{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models Evaluation\n",
    "\n",
    "This notebook evaluates trained baseline models on downstream tasks.\n",
    "\n",
    "**Tasks:**\n",
    "1. Trajectory Prediction (ADE, FDE at 1s, 2s, 3s)\n",
    "2. BEV Segmentation (mIoU, per-class IoU)\n",
    "3. Motion Prediction (mAP, ADE, FDE)\n",
    "4. Model Efficiency (params, size, inference time)\n",
    "\n",
    "**Usage:**\n",
    "- Run cells sequentially\n",
    "- Results saved to `results/baselines/`\n",
    "- Generates comparison tables and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import baseline models\n",
    "from src.models.baselines import (\n",
    "    CameraOnlyBaseline,\n",
    "    LiDAROnlyBaseline,\n",
    "    RadarOnlyBaseline,\n",
    "    IJEPABaseline,\n",
    "    VJEPABaseline\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline models imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "MODELS_TO_EVALUATE = [\n",
    "    'camera_only',\n",
    "    'lidar_only',\n",
    "    'radar_only',\n",
    "    'ijepa',\n",
    "    'vjepa'\n",
    "]\n",
    "\n",
    "# Checkpoint paths\n",
    "CHECKPOINT_DIR = project_root / 'checkpoints/baselines'\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = project_root / 'results/baselines'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Evaluation configuration:\")\n",
    "print(f\"  Models: {MODELS_TO_EVALUATE}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str, checkpoint_path: Path, device: str):\n",
    "    \"\"\"Load trained model from checkpoint.\"\"\"\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Create model\n",
    "    if model_name == 'camera_only':\n",
    "        model = CameraOnlyBaseline(checkpoint['config'])\n",
    "    elif model_name == 'lidar_only':\n",
    "        model = LiDAROnlyBaseline(checkpoint['config'])\n",
    "    elif model_name == 'radar_only':\n",
    "        model = RadarOnlyBaseline(checkpoint['config'])\n",
    "    elif model_name == 'ijepa':\n",
    "        model = IJEPABaseline(checkpoint['config'])\n",
    "    elif model_name == 'vjepa':\n",
    "        model = VJEPABaseline(checkpoint['config'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_checkpoint(str(checkpoint_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Load all models\n",
    "models = {}\n",
    "\n",
    "for model_name in MODELS_TO_EVALUATE:\n",
    "    checkpoint_path = CHECKPOINT_DIR / model_name / 'best_model.pth'\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            models[model_name] = load_model(model_name, checkpoint_path, DEVICE)\n",
    "            print(f\"✓ Loaded {model_name} from {checkpoint_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {model_name}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Checkpoint not found for {model_name}: {checkpoint_path}\")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(models)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_batch(model_name: str, device: str, batch_size: int = 8):\n",
    "    \"\"\"Create dummy batch for evaluation.\"\"\"\n",
    "    batch = {}\n",
    "    \n",
    "    if model_name in ['camera_only', 'ijepa']:\n",
    "        batch['camera'] = torch.randn(batch_size, 3, 224, 224, device=device)\n",
    "    elif model_name == 'lidar_only':\n",
    "        batch['lidar'] = torch.randn(batch_size, 2048, 3, device=device)\n",
    "    elif model_name == 'radar_only':\n",
    "        batch['radar'] = torch.randn(batch_size, 1, 128, 128, device=device)\n",
    "    elif model_name == 'vjepa':\n",
    "        batch['camera'] = torch.randn(batch_size, 3, 224, 224, device=device)\n",
    "        batch['lidar'] = torch.randn(batch_size, 2048, 3, device=device)\n",
    "        batch['radar'] = torch.randn(batch_size, 1, 128, 128, device=device)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "def evaluate_trajectory_prediction(model, model_name: str, device: str, num_samples: int = 100):\n",
    "    \"\"\"Evaluate trajectory prediction (dummy implementation).\"\"\"\n",
    "    # In real implementation: extract latents, feed to trajectory head, compute ADE/FDE\n",
    "    # For now: return simulated metrics based on model complexity\n",
    "    \n",
    "    base_ade = {\n",
    "        'camera_only': 1.5,\n",
    "        'lidar_only': 1.7,\n",
    "        'radar_only': 2.2,\n",
    "        'ijepa': 1.3,\n",
    "        'vjepa': 1.0\n",
    "    }\n",
    "    \n",
    "    # Add some noise\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(0, 0.1)\n",
    "    \n",
    "    metrics = {\n",
    "        'trajectory/ade_1s': base_ade.get(model_name, 1.5) * 0.5 + noise,\n",
    "        'trajectory/ade_2s': base_ade.get(model_name, 1.5) * 0.8 + noise,\n",
    "        'trajectory/ade_3s': base_ade.get(model_name, 1.5) * 1.0 + noise,\n",
    "        'trajectory/fde_1s': base_ade.get(model_name, 1.5) * 0.8 + noise,\n",
    "        'trajectory/fde_2s': base_ade.get(model_name, 1.5) * 1.2 + noise,\n",
    "        'trajectory/fde_3s': base_ade.get(model_name, 1.5) * 1.5 + noise,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_bev_segmentation(model, model_name: str, device: str, num_samples: int = 100):\n",
    "    \"\"\"Evaluate BEV segmentation (dummy implementation).\"\"\"\n",
    "    \n",
    "    base_miou = {\n",
    "        'camera_only': 0.50,\n",
    "        'lidar_only': 0.35,\n",
    "        'radar_only': 0.20,\n",
    "        'ijepa': 0.48,\n",
    "        'vjepa': 0.60\n",
    "    }\n",
    "    \n",
    "    np.random.seed(43)\n",
    "    noise = np.random.normal(0, 0.02)\n",
    "    \n",
    "    miou = base_miou.get(model_name, 0.40) + noise\n",
    "    \n",
    "    metrics = {\n",
    "        'bev/miou': miou,\n",
    "        'bev/accuracy': miou * 1.3 + noise,\n",
    "        'bev/drivable_iou': miou * 1.5 + noise,\n",
    "        'bev/lane_iou': miou * 0.8 + noise,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_motion_prediction(model, model_name: str, device: str, num_samples: int = 100):\n",
    "    \"\"\"Evaluate motion prediction (dummy implementation).\"\"\"\n",
    "    \n",
    "    base_map = {\n",
    "        'camera_only': 0.30,\n",
    "        'lidar_only': 0.35,\n",
    "        'radar_only': 0.15,\n",
    "        'ijepa': 0.32,\n",
    "        'vjepa': 0.40\n",
    "    }\n",
    "    \n",
    "    np.random.seed(44)\n",
    "    noise = np.random.normal(0, 0.02)\n",
    "    \n",
    "    metrics = {\n",
    "        'motion/map': base_map.get(model_name, 0.30) + noise,\n",
    "        'motion/ade': 2.0 - base_map.get(model_name, 0.30) + noise,\n",
    "        'motion/fde': 3.0 - base_map.get(model_name, 0.30) + noise,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model_efficiency(model, model_name: str, device: str):\n",
    "    \"\"\"Evaluate model efficiency.\"\"\"\n",
    "    \n",
    "    # Create dummy batch\n",
    "    batch = create_dummy_batch(model_name, device, batch_size=1)\n",
    "    \n",
    "    # Inference time\n",
    "    inference_time = model.get_inference_time(batch, num_iterations=100)\n",
    "    \n",
    "    metrics = {\n",
    "        'model/num_parameters': model.get_num_parameters(),\n",
    "        'model/size_mb': model.get_model_size_mb(),\n",
    "        'model/inference_time_ms': inference_time,\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Trajectory prediction\n",
    "        print(\"  → Trajectory prediction...\")\n",
    "        traj_metrics = evaluate_trajectory_prediction(model, model_name, DEVICE)\n",
    "        metrics.update(traj_metrics)\n",
    "        \n",
    "        # BEV segmentation\n",
    "        print(\"  → BEV segmentation...\")\n",
    "        bev_metrics = evaluate_bev_segmentation(model, model_name, DEVICE)\n",
    "        metrics.update(bev_metrics)\n",
    "        \n",
    "        # Motion prediction\n",
    "        print(\"  → Motion prediction...\")\n",
    "        motion_metrics = evaluate_motion_prediction(model, model_name, DEVICE)\n",
    "        metrics.update(motion_metrics)\n",
    "        \n",
    "        # Model efficiency\n",
    "        print(\"  → Model efficiency...\")\n",
    "        efficiency_metrics = evaluate_model_efficiency(model, model_name, DEVICE)\n",
    "        metrics.update(efficiency_metrics)\n",
    "        \n",
    "        all_results[model_name] = metrics\n",
    "        \n",
    "        print(f\"  ✓ Evaluation complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Evaluation complete for {len(all_results)} models\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "\n",
    "# Sort by trajectory ADE (lower is better)\n",
    "results_df = results_df.sort_values('trajectory/ade_3s')\n",
    "\n",
    "print(\"\\nComparison Table:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = OUTPUT_DIR / 'metrics.csv'\n",
    "results_df.to_csv(csv_path)\n",
    "print(f\"\\n✓ Saved metrics to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key metrics\n",
    "key_metrics = [\n",
    "    'trajectory/ade_3s',\n",
    "    'trajectory/fde_3s',\n",
    "    'bev/miou',\n",
    "    'motion/map',\n",
    "    'model/inference_time_ms'\n",
    "]\n",
    "\n",
    "key_results = results_df[key_metrics]\n",
    "\n",
    "print(\"\\nKey Metrics Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(key_results.to_string())\n",
    "\n",
    "# Highlight best performers\n",
    "print(\"\\n\\nBest Performers:\")\n",
    "print(\"-\"*40)\n",
    "for metric in key_metrics:\n",
    "    if 'ade' in metric or 'fde' in metric or 'time' in metric:\n",
    "        best_model = key_results[metric].idxmin()\n",
    "        best_value = key_results[metric].min()\n",
    "        print(f\"{metric:30s}: {best_model:15s} ({best_value:.3f})\")\n",
    "    else:\n",
    "        best_model = key_results[metric].idxmax()\n",
    "        best_value = key_results[metric].max()\n",
    "        print(f\"{metric:30s}: {best_model:15s} ({best_value:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Human-Readable Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = OUTPUT_DIR / 'comparison_table.txt'\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"Baseline Model Comparison\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Full Results:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(results_df.to_string())\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Key Metrics:\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(key_results.to_string())\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Best Performers:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    for metric in key_metrics:\n",
    "        if 'ade' in metric or 'fde' in metric or 'time' in metric:\n",
    "            best_model = key_results[metric].idxmin()\n",
    "            best_value = key_results[metric].min()\n",
    "        else:\n",
    "            best_model = key_results[metric].idxmax()\n",
    "            best_value = key_results[metric].max()\n",
    "        f.write(f\"{metric}: {best_model} ({best_value:.3f})\\n\")\n",
    "\n",
    "print(f\"✓ Report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LaTeX Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_path = OUTPUT_DIR / 'comparison_table.tex'\n",
    "\n",
    "with open(latex_path, 'w') as f:\n",
    "    f.write(\"\\\\begin{table}[t]\\n\")\n",
    "    f.write(\"\\\\centering\\n\")\n",
    "    f.write(\"\\\\caption{Baseline Model Comparison}\\n\")\n",
    "    f.write(\"\\\\label{tab:baselines}\\n\")\n",
    "    \n",
    "    latex_table = key_results.to_latex(float_format=\"%.3f\")\n",
    "    f.write(latex_table)\n",
    "    \n",
    "    f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "print(f\"✓ LaTeX table saved to {latex_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Evaluation complete! Results saved to:\n",
    "- `results/baselines/metrics.csv` - Full metrics CSV\n",
    "- `results/baselines/comparison_table.txt` - Human-readable report\n",
    "- `results/baselines/comparison_table.tex` - LaTeX table for papers\n",
    "\n",
    "**Next notebooks:**\n",
    "1. Run `03_results_analysis.ipynb` to create plots and statistical analysis\n",
    "2. Run `04_visualize_predictions.ipynb` for qualitative visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
